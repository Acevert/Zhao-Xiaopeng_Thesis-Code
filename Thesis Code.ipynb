{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b4a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the modules \n",
    "import os \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "os.chdir('E:\\\\Desktop\\\\assignment\\\\supply_chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b833bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_excel(\"output.xlsx\")## import the data\n",
    "input_accounts = pd.read_csv(\"input_accounts_21_10_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c359120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select English tweets\n",
    "df_posts = tweets.loc[tweets['lang']=='en']\n",
    "\n",
    "#import the module and create a list of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "additional  = ['rt','rts','retweet','number','look','read','food','eat','amp','us','email','dm','pls','pleas',\n",
    "              'hi','hey','team','thank','dmu','reach','take','drop','send','uk'] \n",
    "swords = set().union(stopwords.words('english'),additional)\n",
    "#replace all signs and stopwords in tweets and tokenize tweets (tokenization means splitting up a larger body of text into pieces such as words, keywords, phrases, symbols and other elements called tokens; in our case we can split the tweets into words)\n",
    "df_posts.drop_duplicates(subset='text',inplace=True)\n",
    "df_posts.shape\n",
    "df_posts['processed_text'] = df_posts['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ', regex=False)\\\n",
    "          .str.replace('(http\\S+)', ' ', regex=False)\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ', regex=False)\\\n",
    "          .str.replace(' +',' ', regex=False)\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords])\n",
    "\n",
    "#stem tweets to combine similar texts\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "df_posts['stemmed'] = df_posts['processed_text'].apply(lambda x: [ps.stem(i) for i in x if i != ''])\n",
    "#word_count\n",
    "df_posts['word_count'] = df_posts['stemmed'].apply(len)\n",
    "\n",
    "#select  tweets published by JustEatTakeaway(JET)ï¼Œ Uber and Deliveroo \n",
    "JET = df_posts.loc[df_posts['user_id'].isin([272030125,21427907,25320756])]\n",
    "Deliveroo = df_posts.loc[df_posts['user_id'].isin([884794020])]\n",
    "Uber = df_posts.loc[df_posts['user_id'].isin([3100005042,1123948811691470849])]\n",
    "\n",
    "\n",
    "#create dataframe only containts retweets \n",
    "JET_re = JET.loc[JET[\"replied_to_id\"] != -1]\n",
    "Deliveroo_re = Deliveroo.loc[Deliveroo[\"replied_to_id\"] != -1]\n",
    "Uber_re = Uber.loc[Uber[\"replied_to_id\"] != -1]\n",
    "\n",
    "\n",
    "#create dataframe only containts original posts\n",
    "JET_or = JET.loc[JET[\"replied_to_id\"] == -1]\n",
    "Deliveroo_or = Deliveroo.loc[Deliveroo[\"replied_to_id\"] == -1]\n",
    "Uber_or = Uber.loc[Uber[\"replied_to_id\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment score \n",
    "import nltk.sentiment.vader as vd\n",
    "from nltk import download\n",
    "sia = vd.SentimentIntensityAnalyzer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "df_posts['sentiment_score'] = df_posts['stemmed'].apply(lambda x: sum([ sia.polarity_scores(i)['compound'] for i in word_tokenize( ' '.join(x) )]) )\n",
    "\n",
    "df_posts['sent_clasification'] = pd.cut(df_posts['sentiment_score'],\\\n",
    "          [-5,-2, 0, 2 , 5],\\\n",
    "          right=True,\\\n",
    "          include_lowest=True,\\\n",
    "          labels=['snegative', 'negative', 'positive', 'spositive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e3f56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSR-RELATED\n",
    "# define sustainability related content\n",
    "df_posts['innovation'] = np.where(df_posts['text'].str.contains('colla|sustain|waste|plastic|recycle|pollutionResponsibility|Sustainable|Ethics|Accountability|Transparency|Environmental|Social|Charity|Donation|Community|Philanthropy|Giving|Volunteer|Empowerment|Equality|Diversity|Inclusion|Fairness|Ethical|Impactful|Green|Conservation|Sustainability|Humanitarian|Support|Corporate|Engagement|Initiative|Wellness|Stakeholder|Responsible|Welfare|Education|Contribution|Empathy|Empower|Non-profit|Resilience|Advocacy|Transparency|Respect|Partnerships|Progress|Collaboration|Empowering|Trust'), 1, 0)\n",
    "df_posts['innovation']\n",
    "#create sustainability tweets dataframe based on original tweets\n",
    "innovation = df_posts.loc[df_posts['innovation']==1 ]\n",
    "uninnovation = df_posts.loc[df_posts['innovation']!=1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user engagement\n",
    "df_posts['engagement']=df_posts['retweet_count']+df_posts['like_count']+df_posts['reply_count']+df_posts['quote_count']\n",
    "df_posts['log_engage']=np.log(df_posts['engagement']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSR-RELATED\n",
    "# define sustainability related content\n",
    "df_posts['innovation'] = np.where(df_posts['text'].str.contains('colla|sustain|waste|plastic|recycle|pollutionResponsibility|Sustainable|Ethics|Accountability|Transparency|Environmental|Social|Charity|Donation|Community|Philanthropy|Giving|Volunteer|Empowerment|Equality|Diversity|Inclusion|Fairness|Ethical|Impactful|Green|Conservation|Sustainability|Humanitarian|Support|Corporate|Engagement|Initiative|Wellness|Stakeholder|Responsible|Welfare|Education|Contribution|Empathy|Empower|Non-profit|Resilience|Advocacy|Transparency|Respect|Partnerships|Progress|Collaboration|Empowering|Trust'), 1, 0)\n",
    "df_posts['innovation']\n",
    "#create sustainability tweets dataframe based on original tweets\n",
    "innovation = df_posts.loc[df_posts['innovation']==1 ]\n",
    "uninnovation = df_posts.loc[df_posts['innovation']!=1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41ebc58a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiang Bowen\\AppData\\Local\\Temp\\ipykernel_40884\\2502972285.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  JET['innovation'] = np.where(JET['text'].str.contains('colla|sustain|waste|plastic|recycle|pollutionResponsibility|Sustainable|Ethics|Accountability|Transparency|Environmental|Social|Charity|Donation|Community|Philanthropy|Giving|Volunteer|Empowerment|Equality|Diversity|Inclusion|Fairness|Ethical|Impactful|Green|Conservation|Sustainability|Humanitarian|Support|Corporate|Engagement|Initiative|Wellness|Stakeholder|Responsible|Welfare|Education|Contribution|Empathy|Empower|Non-profit|Resilience|Advocacy|Transparency|Respect|Partnerships|Progress|Collaboration|Empowering|Trust'), 1, 0)\n"
     ]
    }
   ],
   "source": [
    "#re\n",
    "df_posts.loc[df_posts['replied_to_id'] == -1, 're'] = 1\n",
    "df_posts.loc[df_posts['replied_to_id'] != -1, 're'] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a3d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create year,month,hour for charts  \n",
    "df_posts['year'] = pd.to_datetime(df_posts['created_at']).dt.year\n",
    "df_posts['month'] = pd.to_datetime(df_posts['created_at']).dt.month\n",
    "df_posts['hour'] = pd.to_datetime(df_posts['created_at']).dt.hour\n",
    "\n",
    "df_posts['busy'] = np.where(df_posts['hour'].isin([14, 16, 15, 17, 18, 19]), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6381c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sustainability tweets dataframe based on original tweets\n",
    "innovation = df_posts.loc[df_posts['innovation']==1 ]\n",
    "uninnovation = df_posts.loc[df_posts['innovation']!=1 ]\n",
    "\n",
    "\n",
    "JET = df_posts.loc[df_posts['user_id'].isin([272030125,21427907,25320756])]\n",
    "Deliveroo = df_posts.loc[df_posts['user_id'].isin([884794020])]\n",
    "Uber = df_posts.loc[df_posts['user_id'].isin([3100005042,1123948811691470849])]\n",
    "\n",
    "\n",
    "#create dataframe only containts retweets \n",
    "JET_re = JET.loc[JET[\"replied_to_id\"] != -1]\n",
    "Deliveroo_re = Deliveroo.loc[Deliveroo[\"replied_to_id\"] != -1]\n",
    "Uber_re = Uber.loc[Uber[\"replied_to_id\"] != -1]\n",
    "\n",
    "\n",
    "#create dataframe only containts original posts\n",
    "JET_or = JET.loc[JET[\"replied_to_id\"] == -1]\n",
    "Deliveroo_or = Deliveroo.loc[Deliveroo[\"replied_to_id\"] == -1]\n",
    "Uber_or = Uber.loc[Uber[\"replied_to_id\"] == -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5495950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression results saved to 'linear_regression_results.docx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from docx import Document\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Perform linear regression\n",
    "results = smf.ols('log_engage ~ similarity + sentiment_score + re + word_count + innovation + busy', data=df_posts).fit()\n",
    "\n",
    "# Get the statistical summary of the regression results\n",
    "summary_table = results.summary().tables[1]\n",
    "summary_data = [list(map(str, row)) for row in summary_table.data[1:]]\n",
    "\n",
    "# Create a DataFrame to store the table data\n",
    "df_summary = pd.DataFrame(summary_data, columns=summary_table.data[0])\n",
    "\n",
    "# Create a Word document\n",
    "doc = Document()\n",
    "\n",
    "# Add the regression results to the Word document\n",
    "doc.add_paragraph('Linear Regression Results')\n",
    "doc.add_paragraph(tabulate(df_summary, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save the Word document\n",
    "doc.save('linear_regression_results.docx')\n",
    "\n",
    "print(\"Linear regression results saved to 'linear_regression_results.docx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5be117d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             log_engage   R-squared:                       0.495\n",
      "Model:                            OLS   Adj. R-squared:                  0.486\n",
      "Method:                 Least Squares   F-statistic:                     54.11\n",
      "Date:                Tue, 06 Jun 2023   Prob (F-statistic):           4.94e-39\n",
      "Time:                        21:26:17   Log-Likelihood:                -339.98\n",
      "No. Observations:                 282   AIC:                             692.0\n",
      "Df Residuals:                     276   BIC:                             713.8\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           0.4199      0.167      2.514      0.013       0.091       0.749\n",
      "similirity         -0.0114      0.064     -0.179      0.858      -0.137       0.114\n",
      "sentiment_score    -0.0437      0.057     -0.771      0.442      -0.155       0.068\n",
      "re                  1.7371      0.128     13.524      0.000       1.484       1.990\n",
      "word_count          0.0229      0.008      2.831      0.005       0.007       0.039\n",
      "busy               -0.0046      0.102     -0.046      0.964      -0.205       0.196\n",
      "==============================================================================\n",
      "Omnibus:                       38.284   Durbin-Watson:                   1.672\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               69.758\n",
      "Skew:                           0.747   Prob(JB):                     7.12e-16\n",
      "Kurtosis:                       4.924   Cond. No.                         79.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = smf.ols('log_engage ~ similirity+sentiment_score+re+word_count+innovation+busy ', data=innovation).fit()\n",
    "print(results.summary())\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "results_df = results.summary().tables[1].as_html()\n",
    "df = pd.read_html(results_df)[0]\n",
    "df.to_excel('regression_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "164b3b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept          0.732237\n",
      "similirity        -1.328147\n",
      "sentiment_score   -0.355364\n",
      "re                -4.571168\n",
      "word_count         0.052372\n",
      "busy               0.461925\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "treatment_group = df_posts[df_posts['innovation'] == 1]\n",
    "control_group = df_posts[df_posts['innovation'] == 0]\n",
    "\n",
    "\n",
    "treatment_results = smf.ols('log_engage ~ similirity+sentiment_score+re+word_count+busy', data=treatment_group).fit()\n",
    "control_results = smf.ols('log_engage ~ similirity+sentiment_score+re+word_count+busy', data=control_group).fit()\n",
    "\n",
    "cate_results = treatment_results.params - control_results.params\n",
    "\n",
    "print(cate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8715e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics saved to user_stats.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "user_ids = [272030125, 21427907, 25320756, 884794020,3100005042, 1123948811691470000]\n",
    "output_file = \"user_stats.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for user_id in user_ids:\n",
    "        user_data = df_posts[df_posts['user_id'] == user_id][['log_engage', 'similirity', 'sentiment_score', 're', 'word_count', 'innovation', 'busy']]\n",
    "        descriptive_stats = user_data.describe()\n",
    "        descriptive_stats.to_excel(writer, sheet_name=f\"User_{user_id}\")\n",
    "\n",
    "print(\"Descriptive statistics saved to user_stats.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.rename(columns={'similirity': 'similarity', 'innovation': 'is_CSR', 'innovation': 'is_CSR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "160bce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_excel(\"df_posts.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "30272277",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF R2 Score: 0.39559319159203876\n",
      "RF RMSE: 0.9789760411785597\n",
      "RF MAE: 0.699327376875404\n",
      "RF Correlation Coefficient: 0.6393154063234263\n",
      "Ada R2 Score: 0.39532646397167237\n",
      "Ada RMSE: 0.9791920307605853\n",
      "Ada MAE: 0.7430054814383398\n",
      "Ada Correlation Coefficient: 0.637625794875185\n",
      "GBRT R2 Score: 0.44037990329454624\n",
      "GBRT RMSE: 0.9420068057436769\n",
      "GBRT MAE: 0.6952706467263758\n",
      "GBRT Correlation Coefficient: 0.6636216872493221\n",
      "BAG R2 Score: 0.3947623904673523\n",
      "BAG RMSE: 0.9796486470127829\n",
      "BAG MAE: 0.7001577978294486\n",
      "BAG Correlation Coefficient: 0.6387803773939683\n",
      "MLP R2 Score: 0.42160557565902324\n",
      "MLP RMSE: 0.9576778399572058\n",
      "MLP MAE: 0.7128945037549868\n",
      "MLP Correlation Coefficient: 0.6505831706728901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "variables = ['similarity', 'sentiment_score', 're', 'word_count', 'innovation', 'busy']\n",
    "\n",
    "# Remove missing values for specified variables\n",
    "df_posts.dropna(subset=variables, inplace=True)\n",
    "\n",
    "# Prepare feature matrix X and target variable log_engage\n",
    "X = df_posts[['similarity', 'sentiment_score', 're', 'word_count', 'innovation', 'busy']]\n",
    "y = df_posts['log_engage']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest regression model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# Fit the Random Forest model on the training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = np.mean(np.abs(y_test - y_pred_rf))\n",
    "correlation_rf = np.corrcoef(y_test, y_pred_rf)[0, 1]\n",
    "\n",
    "# Output results\n",
    "print(\"RF R2 Score:\", r2_rf)\n",
    "print(\"RF RMSE:\", rmse_rf)\n",
    "print(\"RF MAE:\", mae_rf)\n",
    "print(\"RF Correlation Coefficient:\", correlation_rf)\n",
    "\n",
    "# Create an AdaBoosting regression model\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "# Fit the AdaBoosting model on the training set\n",
    "ada_model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "rmse_ada = np.sqrt(mean_squared_error(y_test, y_pred_ada))\n",
    "mae_ada = np.mean(np.abs(y_test - y_pred_ada))\n",
    "correlation_ada = np.corrcoef(y_test, y_pred_ada)[0, 1]\n",
    "\n",
    "# Output results\n",
    "print(\"Ada R2 Score:\", r2_ada)\n",
    "print(\"Ada RMSE:\", rmse_ada)\n",
    "print(\"Ada MAE:\", mae_ada)\n",
    "print(\"Ada Correlation Coefficient:\", correlation_ada)\n",
    "\n",
    "# Create a GBRT regression model\n",
    "gbrt_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "# Fit the GBRT model on the training set\n",
    "gbrt_model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_gbrt = gbrt_model.predict(X_test)\n",
    "\n",
    "r2_gbrt = r2_score(y_test, y_pred_gbrt)\n",
    "rmse_gbrt = np.sqrt(mean_squared_error(y_test, y_pred_gbrt))\n",
    "mae_gbrt = np.mean(np.abs(y_test - y_pred_gbrt))\n",
    "correlation_gbrt = np.corrcoef(y_test, y_pred_gbrt)[0, 1]\n",
    "\n",
    "# Output results\n",
    "print(\"GBRT R2 Score:\", r2_gbrt)\n",
    "print(\"GBRT RMSE:\", rmse_gbrt)\n",
    "print(\"GBRT MAE:\", mae_gbrt)\n",
    "print(\"GBRT Correlation Coefficient:\", correlation_gbrt)\n",
    "\n",
    "# Create a Bagging regression model\n",
    "bagging_model = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "# Fit the Bagging model on the training set\n",
    "bagging_model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "\n",
    "r2_bagging = r2_score(y_test, y_pred_bagging)\n",
    "rmse_bagging = np.sqrt(mean_squared_error(y_test, y_pred_bagging))\n",
    "mae_bagging = np.mean(np.abs(y_test - y_pred_bagging))\n",
    "correlation_bagging = np.corrcoef(y_test, y_pred_bagging)[0, 1]\n",
    "\n",
    "# Output results\n",
    "print(\"BAG R2 Score:\", r2_bagging)\n",
    "print(\"BAG RMSE:\", rmse_bagging)\n",
    "print(\"BAG MAE:\", mae_bagging)\n",
    "print(\"BAG Correlation Coefficient:\", correlation_bagging)\n",
    "\n",
    "# Create a MLP regression model (neural network)\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 100), activation='relu', random_state=42)\n",
    "# Fit the MLP model on the training set\n",
    "mlp_model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "\n",
    "r2_mlp = r2_score(y_test, y_pred_mlp)\n",
    "rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))\n",
    "mae_mlp = np.mean(np.abs(y_test - y_pred_mlp))\n",
    "correlation_mlp = np.corrcoef(y_test, y_pred_mlp)[0, 1]\n",
    "\n",
    "# Output results\n",
    "print(\"MLP R2 Score:\", r2_mlp)\n",
    "print(\"MLP RMSE:\", rmse_mlp)\n",
    "print(\"MLP MAE:\", mae_mlp)\n",
    "print(\"MLP Correlation Coefficient:\", correlation_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "402d487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Feature Importances:\n",
      "similirity : 0.23344426320258369\n",
      "sentiment_score : 0.1590813780925364\n",
      "re : 0.4101056269030394\n",
      "word_count : 0.17054469948978832\n",
      "innovation : 0.005734261209020443\n",
      "busy : 0.021089771103031736\n",
      "GBRT Feature Importances:\n",
      "similirity : 0.012945646898462547\n",
      "sentiment_score : 0.045236702331010535\n",
      "re : 0.7695251443295478\n",
      "word_count : 0.1532522813982502\n",
      "innovation : 0.0004979426859810387\n",
      "busy : 0.01854228235674787\n"
     ]
    }
   ],
   "source": [
    "importances_rf = rf_model.feature_importances_\n",
    "print(\"RF Feature Importances:\")\n",
    "for feature, importance in zip(X.columns, importances_rf):\n",
    "    print(feature, \":\", importance)\n",
    "\n",
    "# Output feature importance scores of the GBRT model\n",
    "importances_gbrt = gbrt_model.feature_importances_\n",
    "print(\"GBRT Feature Importances:\")\n",
    "for feature, importance in zip(X.columns, importances_gbrt):\n",
    "    print(feature, \":\", importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "486dd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.rename(columns={'similirity': 'similarity', 'innovation': 'is_CSR', 'innovation': 'is_CSR'}, inplace=True)\n",
    "df_posts['year'] = pd.to_datetime(df_posts['year'], format='%Y')\n",
    "df_posts['month'] = pd.to_datetime(df_posts['month'], format='%m')\n",
    "df_posts['is_pandemic'] = 0\n",
    "df_posts.loc[(df_posts['year'] >= pd.to_datetime('2021')) & (df_posts['month'] >= pd.to_datetime('2021-01')), 'is_pandemic'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4e4bb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.rename(columns={'re': 'is_re','busy':'busy_time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e43b004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['is_pandemic'] = (df_posts['year'].dt.year >= 2021).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2f093ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    11716\n",
      "0     2258\n",
      "Name: is_pandemic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pandemic_counts = df_posts['is_pandemic'].value_counts()\n",
    "print(pandemic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "983c0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression results saved to 'linear_regression_results.docx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from docx import Document\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Perform linear regression\n",
    "results = smf.ols('log_engage ~ similarity + sentiment_score + re + word_count + is_CSR + busy + is_pandemic', data=df_posts).fit()\n",
    "\n",
    "# Get the statistical summary of the regression results\n",
    "summary_table = results.summary().tables[1]\n",
    "summary_data = [list(map(str, row)) for row in summary_table.data[1:]]\n",
    "\n",
    "# Create a DataFrame to store the table data\n",
    "df_summary = pd.DataFrame(summary_data, columns=summary_table.data[0])\n",
    "\n",
    "# Create a Word document\n",
    "doc = Document()\n",
    "\n",
    "# Add the regression results to the Word document\n",
    "doc.add_paragraph('Linear Regression Results')\n",
    "doc.add_paragraph(tabulate(df_summary, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save the Word document\n",
    "doc.save('linear_regression_results.docx')\n",
    "\n",
    "print(\"Linear regression results saved to 'linear_regression_results.docx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7fd0eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             log_engage   R-squared:                       0.406\n",
      "Model:                            OLS   Adj. R-squared:                  0.406\n",
      "Method:                 Least Squares   F-statistic:                     1591.\n",
      "Date:                Thu, 08 Jun 2023   Prob (F-statistic):               0.00\n",
      "Time:                        15:30:05   Log-Likelihood:                -19212.\n",
      "No. Observations:               13974   AIC:                         3.844e+04\n",
      "Df Residuals:                   13967   BIC:                         3.849e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           0.3656      0.035     10.452      0.000       0.297       0.434\n",
      "similarity          0.0217      0.008      2.848      0.004       0.007       0.037\n",
      "sentiment_score    -0.1388      0.017     -8.198      0.000      -0.172      -0.106\n",
      "is_re               2.2570      0.025     90.552      0.000       2.208       2.306\n",
      "word_count         -0.0180      0.001    -12.237      0.000      -0.021      -0.015\n",
      "busy_time           0.1898      0.016     11.629      0.000       0.158       0.222\n",
      "is_pandemic         0.4240      0.026     16.181      0.000       0.373       0.475\n",
      "==============================================================================\n",
      "Omnibus:                     3139.336   Durbin-Watson:                   1.490\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            15939.457\n",
      "Skew:                           0.994   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.840   Cond. No.                         80.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time+ is_pandemic', data=df_posts).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "699e24c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "                log_engage I log_engage II\n",
      "------------------------------------------\n",
      "Intercept       -0.0616      0.3731***    \n",
      "                (0.2017)     (0.0355)     \n",
      "similarity      -0.0146      0.0208***    \n",
      "                (0.0620)     (0.0077)     \n",
      "sentiment_score -0.0193      -0.1494***   \n",
      "                (0.0555)     (0.0175)     \n",
      "is_re           2.0564***    2.2388***    \n",
      "                (0.1480)     (0.0254)     \n",
      "word_count      0.0261***    -0.0194***   \n",
      "                (0.0079)     (0.0015)     \n",
      "busy_time       -0.0290      0.1931***    \n",
      "                (0.0992)     (0.0165)     \n",
      "is_pandemic     0.5261***    0.4378***    \n",
      "                (0.1304)     (0.0267)     \n",
      "R-squared       0.5232       0.3942       \n",
      "R-squared Adj.  0.5128       0.3939       \n",
      "==========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n"
     ]
    }
   ],
   "source": [
    "treatment_group = df_posts[df_posts['is_CSR'] == 1]\n",
    "control_group = df_posts[df_posts['is_CSR'] == 0]\n",
    "\n",
    "# Regression analysis for the treatment group\n",
    "treatment_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=treatment_group).fit()\n",
    "\n",
    "# Regression analysis for the control group\n",
    "control_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=control_group).fit()\n",
    "\n",
    "# Summarize the regression results\n",
    "results_summary = summary_col([treatment_results, control_results], stars=True)\n",
    "\n",
    "# Print the results to the console\n",
    "print(results_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2bc724f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             log_engage   R-squared:                       0.394\n",
      "Model:                            OLS   Adj. R-squared:                  0.394\n",
      "Method:                 Least Squares   F-statistic:                     1484.\n",
      "Date:                Thu, 08 Jun 2023   Prob (F-statistic):               0.00\n",
      "Time:                        15:58:47   Log-Likelihood:                -18833.\n",
      "No. Observations:               13692   AIC:                         3.768e+04\n",
      "Df Residuals:                   13685   BIC:                         3.773e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           0.3731      0.035     10.518      0.000       0.304       0.443\n",
      "similarity          0.0208      0.008      2.704      0.007       0.006       0.036\n",
      "sentiment_score    -0.1494      0.018     -8.521      0.000      -0.184      -0.115\n",
      "is_re               2.2388      0.025     88.182      0.000       2.189       2.289\n",
      "word_count         -0.0194      0.001    -12.912      0.000      -0.022      -0.016\n",
      "busy_time           0.1931      0.016     11.710      0.000       0.161       0.225\n",
      "is_pandemic         0.4378      0.027     16.378      0.000       0.385       0.490\n",
      "==============================================================================\n",
      "Omnibus:                     3154.772   Durbin-Watson:                   1.501\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16047.991\n",
      "Skew:                           1.021   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.895   Cond. No.                         79.4\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time+ is_pandemic', data=control_group).fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ac209c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "                log_engage I log_engage II\n",
      "------------------------------------------\n",
      "Intercept       -0.0616      0.3731***    \n",
      "                (0.2017)     (0.0355)     \n",
      "similarity      -0.0146      0.0208***    \n",
      "                (0.0620)     (0.0077)     \n",
      "sentiment_score -0.0193      -0.1494***   \n",
      "                (0.0555)     (0.0175)     \n",
      "is_re           2.0564***    2.2388***    \n",
      "                (0.1480)     (0.0254)     \n",
      "word_count      0.0261***    -0.0194***   \n",
      "                (0.0079)     (0.0015)     \n",
      "busy_time       -0.0290      0.1931***    \n",
      "                (0.0992)     (0.0165)     \n",
      "is_pandemic     0.5261***    0.4378***    \n",
      "                (0.1304)     (0.0267)     \n",
      "R-squared       0.5232       0.3942       \n",
      "R-squared Adj.  0.5128       0.3939       \n",
      "==========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "                 Coefficient Difference  Standard Error (Treatment)  \\\n",
      "Intercept                     -0.434663                    0.201663   \n",
      "similarity                    -0.035333                    0.061977   \n",
      "sentiment_score                0.130030                    0.055493   \n",
      "is_re                         -0.182348                    0.147981   \n",
      "word_count                     0.045468                    0.007928   \n",
      "busy_time                     -0.222147                    0.099220   \n",
      "is_pandemic                    0.088253                    0.130389   \n",
      "\n",
      "                 Standard Error (Control)   T-value       P-value  \n",
      "Intercept                        0.035473 -2.122799  3.378822e-02  \n",
      "similarity                       0.007673 -0.565773  5.715571e-01  \n",
      "sentiment_score                  0.017529  2.234372  2.547437e-02  \n",
      "is_re                            0.025388 -1.214496  2.245790e-01  \n",
      "word_count                       0.001500  5.635454  1.779726e-08  \n",
      "busy_time                        0.016493 -2.208642  2.721573e-02  \n",
      "is_pandemic                      0.026732  0.663053  5.073072e-01  \n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Perform regression analysis for the treatment group\n",
    "treatment_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=treatment_group).fit()\n",
    "\n",
    "# Perform regression analysis for the control group\n",
    "control_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=control_group).fit()\n",
    "\n",
    "# Summarize the regression results\n",
    "results_summary = summary_col([treatment_results, control_results], stars=True)\n",
    "\n",
    "# Print the results to the console\n",
    "print(results_summary)\n",
    "\n",
    "# Extract coefficients and standard errors for the treatment group\n",
    "treatment_coefs = treatment_results.params\n",
    "treatment_std_errors = treatment_results.bse\n",
    "\n",
    "# Extract coefficients and standard errors for the control group\n",
    "control_coefs = control_results.params\n",
    "control_std_errors = control_results.bse\n",
    "\n",
    "# Calculate the difference in coefficients between treatment and control groups\n",
    "coef_diff = treatment_coefs - control_coefs\n",
    "\n",
    "# Perform a statistical test to determine the significance of the differences\n",
    "t_values = coef_diff / np.sqrt(treatment_std_errors**2 + control_std_errors**2)\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=len(treatment_group) + len(control_group) - len(coef_diff)))\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "ate_results = pd.DataFrame({'Coefficient Difference': coef_diff, 'Standard Error (Treatment)': treatment_std_errors, 'Standard Error (Control)': control_std_errors, 'T-value': t_values, 'P-value': p_values})\n",
    "\n",
    "# Print the ATE results\n",
    "print(ate_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "06206904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             log_engage   R-squared:                       0.015\n",
      "Model:                            OLS   Adj. R-squared:                  0.006\n",
      "Method:                 Least Squares   F-statistic:                     1.611\n",
      "Date:                Thu, 08 Jun 2023   Prob (F-statistic):              0.202\n",
      "Time:                        17:40:01   Log-Likelihood:                -329.43\n",
      "No. Observations:                 221   AIC:                             664.9\n",
      "Df Residuals:                     218   BIC:                             675.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           2.4182      0.228     10.616      0.000       1.969       2.867\n",
      "similarity         -0.1170      0.133     -0.882      0.379      -0.378       0.144\n",
      "sentiment_score     0.1109      0.073      1.509      0.133      -0.034       0.256\n",
      "==============================================================================\n",
      "Omnibus:                        7.036   Durbin-Watson:                   1.330\n",
      "Prob(Omnibus):                  0.030   Jarque-Bera (JB):                6.814\n",
      "Skew:                          -0.418   Prob(JB):                       0.0331\n",
      "Kurtosis:                       3.207   Cond. No.                         7.48\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Create a new column 'treatment' and set treatment group to 1 and control group to 0 based on 'is_CSR'\n",
    "df_posts['treatment'] = np.where(df_posts['is_CSR'] == 1, 1, 0)\n",
    "\n",
    "# Extract variables for matching\n",
    "matching_variables = ['similarity', 'sentiment_score']\n",
    "\n",
    "# Perform propensity score matching\n",
    "matcher = NearestNeighbors(n_neighbors=1)\n",
    "matcher.fit(df_posts[matching_variables])\n",
    "distances, indices = matcher.kneighbors(df_posts[matching_variables])\n",
    "\n",
    "# Extract indices of matched samples\n",
    "matched_indices = indices[:, 0]\n",
    "\n",
    "# Extract data for matched treatment and control groups\n",
    "matched_treatment_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 1)]\n",
    "matched_control_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 0)]\n",
    "\n",
    "# Perform CATE analysis\n",
    "cate_results = smf.ols('log_engage ~ similarity + sentiment_score', data=matched_treatment_group).fit()\n",
    "\n",
    "# Print CATE results\n",
    "print(cate_results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "caadb211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "                log_engage I log_engage II\n",
      "------------------------------------------\n",
      "Intercept       0.0439       0.1754***    \n",
      "                (0.2787)     (0.0654)     \n",
      "similarity      -0.0889      0.0129       \n",
      "                (0.0753)     (0.0168)     \n",
      "sentiment_score -0.0523      -0.0512*     \n",
      "                (0.0547)     (0.0291)     \n",
      "is_re           2.3001***    2.3347***    \n",
      "                (0.1873)     (0.0411)     \n",
      "word_count      0.0183*      -0.0082***   \n",
      "                (0.0097)     (0.0026)     \n",
      "busy_time       0.0647       0.1964***    \n",
      "                (0.1104)     (0.0338)     \n",
      "is_pandemic     0.6376***    0.6064***    \n",
      "                (0.1559)     (0.0424)     \n",
      "R-squared       0.5048       0.4506       \n",
      "R-squared Adj.  0.4882       0.4498       \n",
      "==========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "                 Coefficient Difference  Standard Error (Treatment)  \\\n",
      "Intercept                     -0.131418                    0.278720   \n",
      "similarity                    -0.101847                    0.075335   \n",
      "sentiment_score               -0.001036                    0.054675   \n",
      "is_re                         -0.034594                    0.187317   \n",
      "word_count                     0.026560                    0.009724   \n",
      "busy_time                     -0.131694                    0.110376   \n",
      "is_pandemic                    0.031196                    0.155888   \n",
      "\n",
      "                 Standard Error (Control)   T-value   P-value  \n",
      "Intercept                        0.065351 -0.459057  0.646216  \n",
      "similarity                       0.016756 -1.319670  0.187014  \n",
      "sentiment_score                  0.029072 -0.016736  0.986648  \n",
      "is_re                            0.041060 -0.180396  0.856850  \n",
      "word_count                       0.002583  2.639789  0.008325  \n",
      "busy_time                        0.033763 -1.140949  0.253953  \n",
      "is_pandemic                      0.042396  0.193105  0.846885  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Create a new column 'treatment' and set treatment group to 1 and control group to 0 based on 'is_CSR'\n",
    "df_posts['treatment'] = np.where(df_posts['is_CSR'] == 1, 1, 0)\n",
    "\n",
    "# Extract variables for matching\n",
    "matching_variables = ['sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']\n",
    "\n",
    "# Perform propensity score matching\n",
    "matcher = NearestNeighbors(n_neighbors=1)\n",
    "matcher.fit(df_posts[matching_variables])\n",
    "distances, indices = matcher.kneighbors(df_posts[matching_variables])\n",
    "\n",
    "# Extract indices of matched samples\n",
    "matched_indices = indices[:, 0]\n",
    "\n",
    "# Extract data for matched treatment and control groups\n",
    "matched_treatment_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 1)]\n",
    "matched_control_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 0)]\n",
    "\n",
    "# Perform regression analysis for treatment group\n",
    "treatment_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=matched_treatment_group).fit()\n",
    "\n",
    "# Perform regression analysis for control group\n",
    "control_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=matched_control_group).fit()\n",
    "\n",
    "# Summarize regression results\n",
    "results_summary = summary_col([treatment_results, control_results], stars=True)\n",
    "\n",
    "# Print results to console\n",
    "print(results_summary)\n",
    "\n",
    "# Extract coefficients and standard errors for treatment and control groups\n",
    "treatment_coefs = treatment_results.params\n",
    "treatment_std_errors = treatment_results.bse\n",
    "\n",
    "control_coefs = control_results.params\n",
    "control_std_errors = control_results.bse\n",
    "\n",
    "# Calculate the difference in coefficients between treatment and control groups\n",
    "coef_diff = treatment_coefs - control_coefs\n",
    "\n",
    "# Perform statistical test to determine the significance of the differences\n",
    "t_values = coef_diff / np.sqrt(treatment_std_errors**2 + control_std_errors**2)\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=len(matched_treatment_group) + len(matched_control_group) - len(coef_diff)))\n",
    "\n",
    "# Create DataFrame to display results\n",
    "ate_results = pd.DataFrame({'Coefficient Difference': coef_diff, 'Standard Error (Treatment)': treatment_std_errors, 'Standard Error (Control)': control_std_errors, 'T-value': t_values, 'P-value': p_values})\n",
    "\n",
    "# Print ATE results\n",
    "print(ate_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c90a6090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "                log_engage I log_engage II\n",
      "------------------------------------------\n",
      "Intercept       -0.0616      0.1754***    \n",
      "                (0.2017)     (0.0654)     \n",
      "similarity      -0.0146      0.0129       \n",
      "                (0.0620)     (0.0168)     \n",
      "sentiment_score -0.0193      -0.0512*     \n",
      "                (0.0555)     (0.0291)     \n",
      "is_re           2.0564***    2.3347***    \n",
      "                (0.1480)     (0.0411)     \n",
      "word_count      0.0261***    -0.0082***   \n",
      "                (0.0079)     (0.0026)     \n",
      "busy_time       -0.0290      0.1964***    \n",
      "                (0.0992)     (0.0338)     \n",
      "is_pandemic     0.5261***    0.6064***    \n",
      "                (0.1304)     (0.0424)     \n",
      "R-squared       0.5232       0.4506       \n",
      "R-squared Adj.  0.5128       0.4498       \n",
      "==========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "                 Coefficient Difference  Standard Error (Treatment)  \\\n",
      "Intercept                     -0.236916                    0.201663   \n",
      "similarity                    -0.027518                    0.061977   \n",
      "sentiment_score                0.031905                    0.055493   \n",
      "is_re                         -0.278226                    0.147981   \n",
      "word_count                     0.034328                    0.007928   \n",
      "busy_time                     -0.225438                    0.099220   \n",
      "is_pandemic                   -0.080371                    0.130389   \n",
      "\n",
      "                 Standard Error (Control)   T-value   P-value  \n",
      "Intercept                        0.065351 -1.117592  0.263801  \n",
      "similarity                       0.016756 -0.428618  0.668222  \n",
      "sentiment_score                  0.029072  0.509279  0.610581  \n",
      "is_re                            0.041060 -1.811704  0.070099  \n",
      "word_count                       0.002583  4.117234  0.000039  \n",
      "busy_time                        0.033763 -2.150985  0.031530  \n",
      "is_pandemic                      0.042396 -0.586191  0.557777  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Create a new column 'treatment' and set treatment group to 1 and control group to 0 based on 'is_CSR'\n",
    "df_posts['treatment'] = np.where(df_posts['is_CSR'] == 1, 1, 0)\n",
    "\n",
    "# Extract variables for matching\n",
    "matching_variables = ['sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']\n",
    "\n",
    "# Perform propensity score matching\n",
    "matcher = NearestNeighbors(n_neighbors=1)\n",
    "matcher.fit(df_posts[matching_variables])\n",
    "distances, indices = matcher.kneighbors(df_posts[matching_variables])\n",
    "\n",
    "# Extract indices of matched samples\n",
    "matched_indices = indices[:, 0]\n",
    "\n",
    "# Extract data for matched treatment and control groups\n",
    "matched_treatment_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 1)]\n",
    "matched_control_group = df_posts.loc[df_posts.index.isin(matched_indices) & (df_posts['treatment'] == 0)]\n",
    "\n",
    "# Perform regression analysis for treatment group\n",
    "treatment_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=matched_treatment_group).fit()\n",
    "\n",
    "# Perform regression analysis for control group\n",
    "control_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=matched_control_group).fit()\n",
    "\n",
    "# Summarize regression results\n",
    "results_summary = summary_col([treatment_results, control_results], stars=True)\n",
    "\n",
    "# Print results to console\n",
    "print(results_summary)\n",
    "\n",
    "# Extract coefficients and standard errors for treatment and control groups\n",
    "treatment_coefs = treatment_results.params\n",
    "treatment_std_errors = treatment_results.bse\n",
    "\n",
    "control_coefs = control_results.params\n",
    "control_std_errors = control_results.bse\n",
    "\n",
    "# Calculate the difference in coefficients between treatment and control groups\n",
    "coef_diff = treatment_coefs - control_coefs\n",
    "\n",
    "# Perform statistical test to determine the significance of the differences\n",
    "t_values = coef_diff / np.sqrt(treatment_std_errors**2 + control_std_errors**2)\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=len(matched_treatment_group) + len(matched_control_group) - len(coef_diff)))\n",
    "\n",
    "# Create DataFrame to display results\n",
    "ate_results = pd.DataFrame({'Coefficient Difference': coef_diff, 'Standard Error (Treatment)': treatment_std_errors, 'Standard Error (Control)': control_std_errors, 'T-value': t_values, 'P-value': p_values})\n",
    "\n",
    "# Print CATE results\n",
    "print(ate_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e7c4c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('df_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b952cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_csv(\"df_posts0611.csv\")## import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3a92b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "                log_engage I log_engage II\n",
      "------------------------------------------\n",
      "Intercept       -0.0812*     0.3359***    \n",
      "                (0.0431)     (0.0601)     \n",
      "similarity      0.0277**     0.0214**     \n",
      "                (0.0111)     (0.0096)     \n",
      "sentiment_score 0.0048       -0.2563***   \n",
      "                (0.0190)     (0.0243)     \n",
      "is_re           1.9355***    2.2608***    \n",
      "                (0.0312)     (0.0371)     \n",
      "word_count      0.0227***    -0.0315***   \n",
      "                (0.0019)     (0.0021)     \n",
      "busy_time       -0.0119      0.2433***    \n",
      "                (0.0208)     (0.0222)     \n",
      "is_pandemic     0.0692**     0.6896***    \n",
      "                (0.0294)     (0.0522)     \n",
      "R-squared       0.6319       0.3490       \n",
      "R-squared Adj.  0.6314       0.3485       \n",
      "==========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "                 Coefficient Difference  Standard Error (Treatment)  \\\n",
      "Intercept                     -0.417080                    0.043062   \n",
      "similarity                     0.006241                    0.011077   \n",
      "sentiment_score                0.261114                    0.019024   \n",
      "is_re                         -0.325361                    0.031229   \n",
      "word_count                     0.054207                    0.001879   \n",
      "busy_time                     -0.255248                    0.020824   \n",
      "is_pandemic                   -0.620365                    0.029391   \n",
      "\n",
      "                 Standard Error (Control)    T-value       P-value  \n",
      "Intercept                        0.060136  -5.639011  1.743446e-08  \n",
      "similarity                       0.009603   0.425748  6.702980e-01  \n",
      "sentiment_score                  0.024341   8.452191  0.000000e+00  \n",
      "is_re                            0.037144  -6.704640  2.096590e-11  \n",
      "word_count                       0.002125  19.110132  0.000000e+00  \n",
      "busy_time                        0.022184  -8.389177  0.000000e+00  \n",
      "is_pandemic                      0.052233 -10.350824  0.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "df_posts['is_JET'] = df_posts['user_id'].isin([272030125, 21427907, 25320756]).astype(int)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import scipy.stats as stats\n",
    "treatment_group = df_posts[df_posts['is_JET'] == 1]\n",
    "control_group = df_posts[df_posts['is_JET'] == 0]\n",
    "\n",
    "# Perform regression analysis for the treatment group\n",
    "treatment_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=treatment_group).fit()\n",
    "\n",
    "# Perform regression analysis for the control group\n",
    "control_results = smf.ols('log_engage ~ similarity + sentiment_score + is_re + word_count + busy_time + is_pandemic', data=control_group).fit()\n",
    "\n",
    "# Summarize the regression results\n",
    "results_summary = summary_col([treatment_results, control_results], stars=True)\n",
    "\n",
    "# Print the results to the console\n",
    "print(results_summary)\n",
    "\n",
    "# Extract coefficients and standard errors for the treatment group\n",
    "treatment_coefs = treatment_results.params\n",
    "treatment_std_errors = treatment_results.bse\n",
    "\n",
    "# Extract coefficients and standard errors for the control group\n",
    "control_coefs = control_results.params\n",
    "control_std_errors = control_results.bse\n",
    "\n",
    "# Calculate the difference in coefficients between the treatment and control groups\n",
    "coef_diff = treatment_coefs - control_coefs\n",
    "\n",
    "# Perform a statistical test to determine the significance of the differences\n",
    "t_values = coef_diff / np.sqrt(treatment_std_errors**2 + control_std_errors**2)\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=len(treatment_group) + len(control_group) - len(coef_diff)))\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "ate_results = pd.DataFrame({'Coefficient Difference': coef_diff, 'Standard Error (Treatment)': treatment_std_errors, 'Standard Error (Control)': control_std_errors, 'T-value': t_values, 'P-value': p_values})\n",
    "\n",
    "# Print the ATE results\n",
    "print(ate_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e524596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics saved to user_stats.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"user_stats.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for is_csr_value in [1, 0]:\n",
    "        user_data = df_posts[df_posts['is_CSR'] == is_csr_value][['log_engage', 'similarity', 'sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']]\n",
    "        descriptive_stats = user_data.describe()\n",
    "        descriptive_stats.to_excel(writer, sheet_name=f\"is_CSR_{is_csr_value}\")\n",
    "\n",
    "    all_data = df_posts[['log_engage', 'similarity', 'sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']]\n",
    "    all_descriptive_stats = all_data.describe()\n",
    "    all_descriptive_stats.to_excel(writer, sheet_name=\"All_Data\")\n",
    "\n",
    "print(\"Descriptive statistics saved to user_stats.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265bbfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics saved to user_stats.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"user_stats_JET.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for is_csr_value in [1, 0]:\n",
    "        user_data = df_posts[df_posts['is_JET'] == is_csr_value][['log_engage', 'similarity', 'sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']]\n",
    "        descriptive_stats = user_data.describe()\n",
    "        descriptive_stats.to_excel(writer, sheet_name=f\"is_JET{is_csr_value}\")\n",
    "\n",
    "    all_data = df_posts[['log_engage', 'similarity', 'sentiment_score', 'is_re', 'word_count', 'busy_time', 'is_pandemic']]\n",
    "    all_descriptive_stats = all_data.describe()\n",
    "    all_descriptive_stats.to_excel(writer, sheet_name=\"All_Data\")\n",
    "\n",
    "print(\"Descriptive statistics saved to user_stats.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0f2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
